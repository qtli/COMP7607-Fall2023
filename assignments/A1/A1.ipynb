{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEd-mtF0PTrv"
   },
   "source": [
    "# Assignment 1\n",
    "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8c9YBduQCI4"
   },
   "source": [
    "## 1 $n$-gram Language Model\n",
    "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) \\stackrel{\\tiny{\\mbox{def}}}{=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMQ_Z1g8QZef"
   },
   "source": [
    "**Write your answer:**\n",
    "\n",
    "\\begin{align}\n",
    "p(\\vec{w}) &= \\frac{C(w_1)}{C(W)} \\cdot \\frac{C(w_1w_2)}{C(w_1)} \\cdot \\frac{C(w_1w_2w_3)}{C(w_1w_2)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "&= \\frac{1}{C(W)} \\cdot \\frac{1}{1} \\cdot \\frac{C{w_1w_2w_3}}{1} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "&= \\frac{C{w_1w_2w_3}}{C(W)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmgExbf1QtCH"
   },
   "source": [
    "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
    "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
    "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm1ZGFIaRPCP"
   },
   "source": [
    "**Write your answer:**\n",
    "\n",
    "\\begin{align}\n",
    "p_{reversed}(\\vec{w}) &=\\frac{C(w_1w_2w_3)}{C(w_2w_3)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_3w_4)} \\dots \\frac{C(w_{n-3}w_{n-2}w_{n-1})}{C(w_{n-2}w_{n-1})} \\cdot \\frac{C(w_{n-2}w_{n-1}w)}{C({w_{n-1}w_n})} \\cdot \\frac{C(w_{n-1}w_n)}{C(w_n)} \\cdot \\frac{C(w_n)}{C(W)} \\\\\n",
    "&= \\frac{C(w_1w_2w_3)}{1} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\cdot \\frac{C(w_3w_4w_5)}{C(w_3w_4)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\cdot \\frac{C(w_{n-1}w_n)}{C(w_{n-1}w_n)} \\cdot \\frac{C(w_n)}{C(w_n)} \\cdot \\frac{1}{C(W)} \\\\\n",
    "&= \\frac{C(w_1w_2w_3)}{C(W)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\cdot \\frac{C(w_3w_4w_5)}{C(w_3w_4)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now, with some simplification, we can show that the formula of $p_{reversed}(\\vec{w})$ is the same as the one of $p(\\vec{w})$ answered in the previous question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQEc5kz4RniG"
   },
   "source": [
    "## 2 $N$-gram Language Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kSwtN79jWgp"
   },
   "outputs": [],
   "source": [
    "!curl -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
    "!curl -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
    "!curl -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9HCVQwqkTc_"
   },
   "source": [
    "### 2.1 Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "# threshold of frequncey to be classifed as <UNK>\n",
    "G_FREQ_THRE = 3\n",
    "G_UNK = \"<UNK>\"\n",
    "G_SOS = \"<s>\"\n",
    "G_EOS = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "G_N = 1\n",
    "G_WORD_TOKENS = []\n",
    "G_VOCAB_TABLE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "id": "-KhFKCzwkaTn"
   },
   "outputs": [],
   "source": [
    "with open(\"data/lm/train.txt\", 'r') as trainFile:\n",
    "    # sentences = [\"<s> \" + line.strip() + \" </s>\" for line in trainFile]\n",
    "    trainData = [line.strip() for line in trainFile]\n",
    "    \n",
    "with open(\"data/lm/dev.txt\", 'r') as devFile:\n",
    "    devData = [line.strip() for line in devFile]\n",
    "    \n",
    "with open(\"data/lm/test.txt\", 'r') as testFile:\n",
    "    testData = [line.strip() for line in testFile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training datset, and get sentence tokenization out of the training dataset, also add starting/end symbols for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCustomWordTokens(sentence):\n",
    "    pattern = r'<\\w+>|[#$\"\\(\\)\\',.!-]|\\d+(?:,\\d{3})*(?:\\.\\d+)?|\\w+(?:-\\w+)*|\\S+'\n",
    "    G_VOCAB_TABLE\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    wordTokens = tokenizer.tokenize(sentence)\n",
    "    return wordTokens\n",
    "\n",
    "def getNLTKWordTokens(sentence):\n",
    "    wordTokens = word_tokenize(sentence)\n",
    "    return wordTokens\n",
    "\n",
    "def tokenize(sentences):\n",
    "    word_tokens = []\n",
    "    for sentence in sentences:\n",
    "        # add SOS\n",
    "        result = [G_SOS] * max(1, (G_N - 1))\n",
    "        # tokenization\n",
    "        tokens = getNLTKWordTokens(sentence)\n",
    "        # concatenate tokens and EOS\n",
    "        result.extend(tokens)\n",
    "        result.append(G_EOS)\n",
    "        # formulation\n",
    "        word_tokens.extend(result)\n",
    "    return word_tokens\n",
    "        \n",
    "def initLM(n = 1):\n",
    "    global G_TOKENIZED_SENTS, G_WORD_TOKENS, G_VOCAB_TABLE, G_N \n",
    "    G_TOKENIZED_SENTS = []\n",
    "    G_WORD_TOKENS = []\n",
    "    G_VOCAB_TABLE = {}\n",
    "    G_N = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokenization\n",
    "initLM()\n",
    "G_WORD_TOKENS = tokenize(trainData)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1273123 tokens in the raw training dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(G_WORD_TOKENS), \"tokens in the raw training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the training dataset tokenized, we can build out vocabulary by creating a frequency table out of the tokens, and convert tokens appeared less than 3 times to `<UNK>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov(tokens):    \n",
    "    # create a vocab that contains frequnce information of each token\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    # replae tokens with UNK if their freq is too low\n",
    "    return [token if vocab[token] >= G_FREQ_THRE else G_UNK for token in tokens]\n",
    "    \n",
    "G_WORD_TOKENS = oov(G_WORD_TOKENS)\n",
    "G_VOCAB_TABLE = nltk.FreqDist(G_WORD_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final vocabulary size is 17552\n",
      "<s>: 50000\n",
      "facebook: 692\n",
      "has: 6071\n",
      "released: 529\n",
      "a: 31132\n",
      "report: 312\n",
      "that: 10032\n",
      "shows: 289\n",
      "what: 607\n",
      "content: 377\n"
     ]
    }
   ],
   "source": [
    "print(\"The final vocabulary size is\", len(G_VOCAB_TABLE.keys()))\n",
    "\n",
    "demo = list(G_VOCAB_TABLE.items())[:10]\n",
    "for key, val in demo:\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLQNsUA5kfZe"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "The vocabulary size is `17658` after inserting `SOS/EOS` and replacing less frequent tokens with `UNK`. \n",
    "\n",
    "1. `_N` is an important Hyperparameter specifies the window size of the model.\n",
    "\n",
    "/TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJzDNMVikkeX"
   },
   "source": [
    "### 2.2 $N$-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxkcs2HykuR2"
   },
   "source": [
    "To genralize the process of building n-gram models, we'll create a class that contains all the functions we used in previous section as class methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDict(dic, n):\n",
    "    for key, val in dic.items():\n",
    "        if n == 0:\n",
    "            break\n",
    "        print(f\"[{key}: {val}]\", end=\" \")\n",
    "        n -= 1\n",
    "\n",
    "class LM:\n",
    "    # parameters\n",
    "    _N = 1\n",
    "    # all individual word tokens\n",
    "    _word_tokens = []\n",
    "    # a dict that stores all the frequence of indivicual tokens\n",
    "    _vocab = {}\n",
    "    \n",
    "    _ngrams = []\n",
    "    # a probability table of all ngrams\n",
    "    _model = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self, train_sentences, n = 1):\n",
    "        self._N = max(1, n)\n",
    "        # tokenize a list of sentences into a single giant list of tokens with SOS/EOS symbols.\n",
    "        tokens = self._tokenize(train_sentences)\n",
    "        # update with UNK\n",
    "        tokens = self._oov(tokens)\n",
    "        self._word_tokens = tokens\n",
    "        self._vocab = nltk.FreqDist(self._word_tokens)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Create ngram model without doing any smoothing.\n",
    "        \n",
    "        This method purely calculates the probability of a given ngram based on\n",
    "        the frequence of the given ngram and the frequence of its predecessor.\n",
    "        \n",
    "        Returns:\n",
    "            dict: the probability table of ngrams.\n",
    "        \"\"\"\n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        \n",
    "\n",
    "        # create grams with window size n - 1\n",
    "        n_1_grams = nltk.ngrams(self._word_tokens, self._N-1)\n",
    "        n_1_vocab = nltk.FreqDist(n_1_grams)\n",
    "\n",
    "        # for unigram\n",
    "        if self._N == 1:\n",
    "            tokenSize = len(self._word_tokens)\n",
    "            self._model = {token: freq / tokenSize for token, freq in n_vocab.items()}\n",
    "        else:\n",
    "            # create the probability table for ngram\n",
    "            self._model = {ngram: freq / n_1_vocab[ngram[:-1]] for ngram, freq in n_vocab.items()}\n",
    "    \n",
    "    def getModel(self):\n",
    "        return self._model\n",
    "        \n",
    "            \n",
    "    def perplexity(self, testData):\n",
    "        # toknization\n",
    "        testTokens = self._tokenize(testData)\n",
    "        # replace oov with UNK\n",
    "        kownVocab = self._vocab.keys()\n",
    "        testTokens = [token if token in kownVocab else G_UNK for token in testTokens]\n",
    "        # get test grams to calculate probabilities\n",
    "        testGrams = list(nltk.ngrams(testTokens, self._N))\n",
    "        \n",
    "        logs = [math.log(self._calcProb(ngram)) for ngram in testGrams]\n",
    "        return math.exp((-1 / len(testTokens)) * sum(logs))\n",
    "    \n",
    "    def _calcProb(self, ngram):\n",
    "        \"\"\"Calculate the probability of a given ngram based on current model\n",
    "\n",
    "        Args:\n",
    "            ngram: the traget.\n",
    "        Returns:\n",
    "            float: the probability\n",
    "        \"\"\"\n",
    "        # check if the ngram is known\n",
    "        return self._model.get(ngram, 0)\n",
    "    \n",
    "    def peekTokens(self, n):\n",
    "        \"\"\"Have a peek at the tokens\n",
    "        \n",
    "        Args:\n",
    "            n: the number of elements to print.\n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "            print(self._word_tokens[i], end=\" \")\n",
    "    \n",
    "    def peekVocab(self, n):\n",
    "        printDict(self._vocab, n)\n",
    "            \n",
    "    def getVocabSize(self):\n",
    "        return len(sel._vocab)\n",
    "    \n",
    "    def peekModel(self, n):\n",
    "        \"\"\"Have a peek at the ngram table created\n",
    "        \n",
    "        Args:\n",
    "            n: the number of elements to print.\n",
    "        \"\"\"\n",
    "        printDict(self._model, n)\n",
    "        \n",
    "    def _tokenize(self, sentences):\n",
    "        \"\"\"Transform a list of sentences into a list of tokens.\n",
    "        \n",
    "        This function noly only tokenize the input, but also add SOS and EOS symbol before\n",
    "        and after each sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentences: A list of sentences: str, expected to be the training data.\n",
    "            \n",
    "        Returns:\n",
    "            list: A single giant list of tokens that appeared in the input, with the same order\n",
    "                  as the input.\n",
    "        \"\"\"\n",
    "        word_tokens = []\n",
    "        for sentence in sentences:\n",
    "            # add SOS\n",
    "            result = [G_SOS] * max(1, (self._N - 1))\n",
    "            # tokenization\n",
    "            tokens = self._getCustomWordTokens(sentence)\n",
    "            # concatenate tokens and EOS\n",
    "            result.extend(tokens)\n",
    "            result.append(G_EOS)\n",
    "            # formulation\n",
    "            word_tokens.extend(result)\n",
    "        return word_tokens\n",
    "    \n",
    "        \n",
    "        \n",
    "    def _oov(self, tokens):\n",
    "        \"\"\"This function replace any tokens with low frequence of appearence with UNK\n",
    "        \n",
    "        Args:\n",
    "            tokens: word tokens\n",
    "        \"\"\"\n",
    "        # create a vocab that contains frequnce information of each token\n",
    "        vocab = nltk.FreqDist(tokens)\n",
    "        # replae tokens with UNK if their freq is too low\n",
    "        return [token if vocab[token] >= G_FREQ_THRE else G_UNK for token in tokens]\n",
    "    \n",
    "    def _getCustomWordTokens(self, sentence):\n",
    "        pattern = r'<\\w+>|[#$\"\\(\\)\\',.!-]|\\d+(?:,\\d{3})*(?:\\.\\d+)?|\\w+(?:-\\w+)*|\\S+'\n",
    "        G_VOCAB_TABLE\n",
    "        tokenizer = RegexpTokenizer(pattern)\n",
    "        wordTokens = tokenizer.tokenize(sentence)\n",
    "        return wordTokens\n",
    "\n",
    "    def _getNLTKWordTokens(self, sentence):\n",
    "        wordTokens = word_tokenize(sentence)\n",
    "        return wordTokens\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to methods already used, we add `naive_createNgrams` method in the class. This method will create frequence table for **ngram** as well as **n-1gram**. Based on the formula we've discussed in section1, we know the probability of a given ngram is $\\frac{C(w_{N-1} \\dots w_{n-2}w_{n-1}w)}{C(w_{N-1} \\dots w_{n-2}w_{n-1})}$ where the denominator is frequence of the **n-1gram** and the nominator is the frequence of the given **ngram**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',): 0.039273503031521696] [('facebook',): 0.0005435452819562603] [('has',): 0.0047685887380873645] [('released',): 0.0004155136620734996] [('a',): 0.02445325392754667] [('report',): 0.0002450666589166954] [('that',): 0.007879835648244514] [('shows',): 0.00022700084752219542] [('what',): 0.0004767803268026734] [('content',): 0.0002961222128576736] "
     ]
    }
   ],
   "source": [
    "unigram = LM(trainData)\n",
    "unigram.train()\n",
    "unigram.peekModel(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model created without using any smoothing technics, let's try to analyze its perplxity againt the training data and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 796.9548612984283\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the training dataset is:\", unigram.perplexity(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 744.4893673514492\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the dev dataset is:\", unigram.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'facebook'): 0.00538] [('facebook', 'has'): 0.11271676300578035] [('has', 'released'): 0.015977598418711908] [('released', 'a'): 0.1720226843100189] [('a', 'report'): 0.003950918668893743] [('report', 'that'): 0.041666666666666664] [('that', 'shows'): 0.006379585326953748] [('shows', 'what'): 0.03460207612456748] [('what', 'content'): 0.0016474464579901153] [('content', 'was'): 0.007957559681697613] "
     ]
    }
   ],
   "source": [
    "bigram = LM(sentences, 2)\n",
    "bigram.train()\n",
    "bigram.peekModel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 48.442156606470306\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the training dataset is:\", bigram.perplexity(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[809], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe perplexity of this model against the dev dataset is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mbigram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevData\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[542], line 71\u001b[0m, in \u001b[0;36mLM.perplexity\u001b[0;34m(self, testData)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# get test grams to calculate probabilities\u001b[39;00m\n\u001b[1;32m     69\u001b[0m testGrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mngrams(testTokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N))\n\u001b[0;32m---> 71\u001b[0m logs \u001b[38;5;241m=\u001b[39m [math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calcProb(ngram)) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m testGrams]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mexp((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(testTokens)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(logs))\n",
      "Cell \u001b[0;32mIn[542], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# get test grams to calculate probabilities\u001b[39;00m\n\u001b[1;32m     69\u001b[0m testGrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mngrams(testTokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N))\n\u001b[0;32m---> 71\u001b[0m logs \u001b[38;5;241m=\u001b[39m [\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calcProb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m testGrams]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mexp((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(testTokens)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(logs))\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the dev dataset is:\", bigram.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3o9Nez8kvYm"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "By running the perplexity analysis againt the dev dataset, **there's an error occured** complaining the math domain error. This error demonstrates a problem of our implementation of creating ngrams probability table. **That is, it can't handle unseen data**. When an unseen ngram occurs in the test dataset the probability of it will be `0`. Hence, during calculating the perplexity, we'll have `log(0)` in the equation, which is an undefined behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOQUqM73kzf-"
   },
   "source": [
    "### 2.3 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LgXRmJwk3Y-"
   },
   "source": [
    "#### 2.3.1 Add-one (Laplace) smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_ADD_K(LM):\n",
    "    _K = 1\n",
    "    # model related\n",
    "    _subVocab = {}\n",
    "    def __init__(self, sentences, n=1):\n",
    "        super().__init__(sentences, n)\n",
    "    \n",
    "    def train(self, k = 1):\n",
    "        self._K = k\n",
    "        self._smoothing()\n",
    "        \n",
    "    def _smoothing(self):\n",
    "        \"\"\"Create a ngram model with add_k_smoothing.\n",
    "        \n",
    "        When k is 1, this function creates model with laplace smoothing.\n",
    "        \n",
    "        Agrs:\n",
    "            k: the increment amount for freq of each ngrams.\n",
    "        \"\"\"\n",
    "        # unigram doesn't support smoothing\n",
    "        if self._N == 1:\n",
    "            print(\"unigram model doesn't support smoothing, continue without smoothing...\")\n",
    "            return super().train()\n",
    "        \n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        \n",
    "        # create grams with window size n - 1\n",
    "        n_1_grams = nltk.ngrams(self._word_tokens, self._N-1)\n",
    "        n_1_vocab = nltk.FreqDist(n_1_grams)\n",
    "        self._subVocab = n_1_vocab\n",
    "        \n",
    "        vocabSize = len(self._vocab)\n",
    "        self._model = {ngram: (freq + self._K) / (n_1_vocab[ngram[:-1]] + self._K * vocabSize) for ngram, freq in n_vocab.items()}        \n",
    "    \n",
    "    \n",
    "    def _calcProb(self, ngram):\n",
    "        \"\"\"Calculate the probability of a given ngram based on current model\n",
    "        \n",
    "        When ngram is known, directly return the probability\n",
    "        When ngram is unkown and smoothing is not used, return 0\n",
    "        When ngram is unknown but smoothing is used, return smoothed value.\n",
    "        \n",
    "        Args:\n",
    "            ngram: the traget.\n",
    "        \"\"\"\n",
    "        # check if the ngram is known         \n",
    "        prob = self._model.get(ngram, 0)\n",
    "        \n",
    "        if prob == 0:    \n",
    "            vocabSize = len(self._vocab)\n",
    "            subFreq = self._subVocab.get(ngram[:-1], 0)\n",
    "            return self._K / (subFreq + self._K * vocabSize)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "id": "lFG7jCIRk7Qw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'facebook'): 0.003996920890573188] [('facebook', 'has'): 0.0043301907476430606] [('has', 'released'): 0.004148499343859797] [('released', 'a'): 0.005088214147447597] [('a', 'report'): 0.002547038041245584] [('report', 'that'): 0.0007836990595611285] [('that', 'shows'): 0.0023564385150812066] [('shows', 'what'): 0.000616557367860546] [('what', 'content'): 0.00011013822347045542] [('content', 'was'): 0.00022310223659992193] "
     ]
    }
   ],
   "source": [
    "bigram_laplace = LM_ADD_K(trainData, 2)\n",
    "bigram_laplace.train()\n",
    "bigram_laplace.peekModel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 678.3425847140647\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the training dataset is:\", bigram_laplace.perplexity(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 829.2794789075276\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the dev dataset is:\", bigram_laplace.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36yTKPXFk8f2"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "With smoothing where some probability mass is distributed to all combinations of unseen ngrams, we no longer have the issue discussed in last section. Therefore, we can observe the perplexity of the model against the dev dataset is `829.27`. However, the perplexity of the new model againt the training dataset is much higher than the previous model without smoothing. This case happens because the **training dataset contains only ngrams the model knows**, meaning it expects the probability mass on ngrams in the training dataset to be as much as possible. However, after smoothing, some probability mass is **donated** to unseen ngrams, which is not helping in this case as we'll never encounter unseen ngrams in the training dataset and those mass are sort of **wasted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8cFbczqlBR_"
   },
   "source": [
    "#### 2.3.2: Add-$k$ smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "id": "uV_ZiAgIlPUu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 459.10639016556837\n",
      "The perplexity of this model against the dev dataset is: 608.8950847554092\n"
     ]
    }
   ],
   "source": [
    "bigram_add_k = LM_ADD_K(trainData, 2)\n",
    "bigram_add_k.train(0.5)\n",
    "print(\"The perplexity of this model against the training dataset is:\", bigram_add_k.perplexity(trainData))\n",
    "print(\"The perplexity of this model against the dev dataset is:\", bigram_add_k.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 199.41218240875557\n",
      "The perplexity of this model against the dev dataset is: 338.2739273653482\n"
     ]
    }
   ],
   "source": [
    "bigram_add_k.train(0.1)\n",
    "print(\"The perplexity of this model against the training dataset is:\", bigram_add_k.perplexity(trainData))\n",
    "print(\"The perplexity of this model against the dev dataset is:\", bigram_add_k.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 55.751861780817016\n",
      "The perplexity of this model against the dev dataset is: 217.6393120902775\n"
     ]
    }
   ],
   "source": [
    "bigram_add_k.train(0.001)\n",
    "print(\"The perplexity of this model against the training dataset is:\", bigram_add_k.perplexity(trainData))\n",
    "print(\"The perplexity of this model against the dev dataset is:\", bigram_add_k.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHFNf8OIlQ0O"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "Using add-k-smoothing, the perplexity of the model is less than the one with laplace. Especially, when k is getting lower. The reason lower k generates better result can be the follwings. Given a unigram, if the size of its corresponding bigram is limited, after smoothing, the probability of the original seen bigram will be spreaded drastically. Therefore, by lowering the factor `k`, we can reduce the amount of probability mass spreaded to unseen data, hence remain the dominance of the seen bigrams in the probability mass. On the other hand, observing that keep lowering the factor `k`, the perplexity againt the dev dataset is also reducing, we can also infer that the dev dataset is similar to the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjKEO_TqlUrX"
   },
   "source": [
    "#### 2.3.3 Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {
    "id": "pcdd4cvYlZuO"
   },
   "outputs": [],
   "source": [
    "class LM_LINEAR_INTERPOLATION(LM):\n",
    "    _LAMBDAS = []\n",
    "    _PROB_BI = {}\n",
    "    _PROB_UNI = {}\n",
    "    _RAW_DATA = None\n",
    "    def __init__(self, sentences, n):\n",
    "        super().__init__(sentences, n)\n",
    "        self._RAW_DATA = sentences\n",
    "        self._model = []\n",
    "        self._LAMBDAS = [1/self._N]*self._N\n",
    "        \n",
    "    \n",
    "    def trainWithUniformWordTokens(self):\n",
    "        \"\"\"\n",
    "        All submodels in this training function will use the same wordTokens which is generated by the highest order model.\n",
    "        The potential problem is that SOS/EOS are introduced multiple times in the highest order model. EOS/SOS tokens will dominate\n",
    "        in the lower oder models. \n",
    "        Args:\n",
    "            ls: a list of lambdas in sequential order L0...LN, if not provided, automatically generated with equal weights.\n",
    "        \"\"\"\n",
    "        if self._N == 1:\n",
    "            print(\"unigram model doesn't support smoothing, continue without smoothing...\")\n",
    "            return super().train()\n",
    "        super().__init__(self._RAW_DATA, self._N)\n",
    "        \n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        for i in range(1, len(self._LAMBDAS)):\n",
    "            # create grams with window size n - i\n",
    "            n_1_grams = nltk.ngrams(self._word_tokens, self._N-i)\n",
    "            n_1_vocab = nltk.FreqDist(n_1_grams)\n",
    "            prob_n_gram = {ngram: (freq) / (n_1_vocab[ngram[:-1]]) for ngram, freq in n_vocab.items()}\n",
    "            self._model.append(prob_n_gram)\n",
    "            n_vocab = n_1_vocab\n",
    "        \n",
    "        tokenSize = len(self._word_tokens)\n",
    "        prob_uni_gram =  {token: freq / tokenSize for token, freq in n_vocab.items()}\n",
    "        self._model.append(prob_uni_gram)\n",
    "        # each time the probability is appened, so that higher-order models are in the front\n",
    "        # so reverse the _model\n",
    "        self._model.reverse()\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ls: a list of lambdas in sequential order L0...LN, if not provided, automatically generated with equal weights.\n",
    "        \"\"\"\n",
    "        if self._N == 1:\n",
    "            print(\"unigram model doesn't support smoothing, continue without smoothing...\")\n",
    "            return super().train()\n",
    "        self._model = []\n",
    "        \n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        for i in range(1, self._N + 1):\n",
    "            print(f\"training {i}gram model...\")\n",
    "            subModel = LM(self._RAW_DATA, i)\n",
    "            subModel.train()\n",
    "            self._model.append(subModel.getModel())\n",
    "        \n",
    "    def optimize(self, data, n_iter, learning_rate):\n",
    "        \"\"\"Optimize the lambda parameters using gradient descent.\n",
    "        \n",
    "        This method optimize by maximizing the probability of ngrams in the data, instead of minimizing the perpleixty.\n",
    "        This methods is explained in https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58.\n",
    "        \n",
    "        Agrs:\n",
    "            data: the data we wish to optimize the lambds upon\n",
    "            n_iter: the number of iterations to perform in finding the optimal lambdas.\n",
    "            learning_rate: the rate of descent for each iteration.\n",
    "        Returns:\n",
    "            list: a list of optimal lambdas for unigram ... ngram.\n",
    "        \"\"\"\n",
    "        # RESET LAMBDAS\n",
    "        self._LAMBDAS = [1/self._N]*self._N\n",
    "        \n",
    "        # toknization\n",
    "        optTokens = self._tokenize(data)\n",
    "        # replace oov with UNK\n",
    "        kownVocab = self._vocab.keys()\n",
    "        optTokens = [token if token in kownVocab else G_UNK for token in optTokens]\n",
    "        # get opt grams to calculate probabilities\n",
    "        optGrams = list(nltk.ngrams(optTokens, self._N))\n",
    "        \n",
    "        # get the probability matrix\n",
    "        probMatrix = self._probMatrix(optGrams)\n",
    "        \n",
    "        ngram_probs = probMatrix[:, 1:]\n",
    "        uniform_prob = probMatrix[:, [0]]\n",
    "        \n",
    "        weights = np.array(self._LAMBDAS)\n",
    "        \n",
    "        for iteration in range(n_iter):\n",
    "            # 2. Calculate gradients for each n-gram model\n",
    "            interpolated_probs = np.sum(probMatrix * weights, axis=1, keepdims=True)\n",
    "            ngram_gradients = np.mean((ngram_probs - uniform_prob) / interpolated_probs, axis=0)\n",
    "\n",
    "            # 3. Update interpolation weights for all models\n",
    "            weights[1:] += learning_rate * ngram_gradients\n",
    "            weights[0] = 1 - weights[1:].sum()\n",
    "        return weights\n",
    "    \n",
    "    def updateLambdas(self, ls):\n",
    "        if ls is None:\n",
    "            return\n",
    "        # if (sum(ls) != 1):\n",
    "        #     print(\"Incorrect lambdas, sum not equal to 1\")\n",
    "        #     return\n",
    "        if (len(ls) != self._N):\n",
    "            print(\"Incorrect lambda length\")\n",
    "            return\n",
    "        self._LAMBDAS = ls\n",
    "        \n",
    "    \n",
    "    def _probMatrix(self, ngrams):\n",
    "        # a probability matrix of grams x models\n",
    "        matrix = np.zeros((len(ngrams), self._N))\n",
    "        for i in range(len(self._model)):\n",
    "            matrix[:, [i]] = np.array([self._model[i].get(ngram[self._N - i - 1:], 0) for ngram in ngrams]).reshape(-1, 1)\n",
    "        print(\"A glimpse at the probability matrix of the data optimized upon...\")\n",
    "        print(matrix[:5, :5])\n",
    "        return matrix\n",
    "        \n",
    "    def _calcProb(self, ngram):\n",
    "        prob = 0\n",
    "        # len(self._model) = self._N\n",
    "        for i in range(self._N):\n",
    "            # model stores prob distributions from 0 to N.\n",
    "            prob += self._model[i].get(ngram[self._N - 1 - i:], 0) * self._LAMBDAS[i]\n",
    "        if (prob == 0):\n",
    "            print(\"calcProb error: not found\", ngram)\n",
    "        return prob\n",
    "    \n",
    "    def peekModel(self, n):\n",
    "        for probs in self._model:\n",
    "            printDict(probs, n)\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a trigram model smoothed by linear interpolation with equal weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1gram model...\n",
      "training 2gram model...\n",
      "training 3gram model...\n",
      "Have a look at the submodels:\n",
      "[('<s>',): 0.039273503031521696] [('facebook',): 0.0005435452819562603] [('has',): 0.0047685887380873645] \n",
      "[('<s>', 'facebook'): 0.00538] [('facebook', 'has'): 0.11271676300578035] [('has', 'released'): 0.015977598418711908] \n",
      "[('<s>', '<s>', 'facebook'): 0.00538] [('<s>', 'facebook', 'has'): 0.24907063197026022] [('facebook', 'has', 'released'): 0.02564102564102564] \n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation = LM_LINEAR_INTERPOLATION(trainData, 3)\n",
    "trigram_linear_interpolation.train()\n",
    "print(\"Have a look at the submodels:\")\n",
    "trigram_linear_interpolation.peekModel(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 12.898964375272874\n",
      "The perplexity of this model against the dev dataset is: 120.72354673770525\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this model against the training dataset is:\", trigram_linear_interpolation.perplexity(trainData))\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try manually to optimize the lambdas to perform best on dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 132.4205743334757\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.3, 0.2, 0.5])\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 122.83927845909868\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.2, 0.4, 0.4])\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 127.17716778785392\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.2, 0.7, 0.1])\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 trials, we decide to use $\\lambda_1 = 0.2, \\lambda_2 = 0.4, \\lambda_3 = 0.4$ for our manually optimized weights to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the test dataset is: 121.94002570853857\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.2, 0.4, 0.4])\n",
    "print(\"The perplexity of this model against the test dataset is:\", trigram_linear_interpolation.perplexity(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "After using linear interpolation, we can find the perplexity drastically reduced compared to add-k smoothing. When training a ngram model, we always wish to increase `n` as it will give us more history information in predicting. However, as `n` increases, the model will be hungry for data, and withou enough data, the probability table of model will be sparse, leading worse performance than lower order models. Linear interpolation gives us a flexibility to take context advantages of higher order model and mitigate its sparsity problem with lower order models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the lambda parameters based on the dev dataset using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glimpse at the probability matrix of the data optimized upon...\n",
      "[[1.68870555e-04 2.16000000e-03 4.32000000e-03]\n",
      " [4.68308754e-03 3.18181818e-02 3.24074074e-02]\n",
      " [4.75907929e-05 1.14735289e-03 0.00000000e+00]\n",
      " [2.38852049e-02 3.22580645e-02 0.00000000e+00]\n",
      " [2.17344081e-02 6.08991869e-02 0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.24624052, 0.51720745, 0.23655203])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_linear_interpolation.optimize(devData, 500, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the lambdas and see if the perplexity against dev is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_linear_interpolation.updateLambdas([0.24624052, 0.51720745, 0.23655203])\n",
    "print(\"The perplexity of this model against the training dataset is:\", bigram_linear_interpolation.perplexity(sentences))\n",
    "devPPL = bigram_linear_interpolation.perplexity(devData)\n",
    "print(\"The perplexity of this model against the dev dataset is:\", devPPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyKqmQ37lcH2"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "\\# todo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzSbk2bClf3u",
    "tags": []
   },
   "source": [
    "##### **Optimization**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, optimize the lambdas using [Gradient Descent](https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58). Gradient Descent is normally used for unconstrained optimization problems, and this time, we have a constraint that the sum of the weights should be `1`. To mitigate this, we'll set the weight of the unigram to be $1 - \\lambda_2 - \\lambda_3$. Then, the formula of the probability can be writen as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P &= \\frac{1}{N_{word}} \\sum_{word} log P(word) \\\\\n",
    "  &= \\frac{1}{N_{word}} \\sum_{word} log ((1 - \\lambda_2 - \\lambda_3)P(w) + (\\lambda_2 P(w | w_{-1})) + (\\lambda_3 P(w | w_{-1}w_{-2})))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To differentiate upon each lambda, we can have the formula for $\\lambda_j$ as the followings:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta P}{\\delta \\lambda_j} &= \\frac{1}{N_{word}} \\sum_{word} \\frac{1}{P(word)} \\frac{\\delta P(word)}{\\delta \\lambda_j} \\\\\n",
    "&= \\frac{1}{N_{word}} \\sum_{word} \\frac{1}{P(word)}(-P(w) + P(w | w_{\\dots}w_{j-1}) )\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glimpse at the probability matrix of the data optimized upon...\n",
      "[[1.72803413e-04 4.32000000e-03 4.32000000e-03]\n",
      " [4.76858874e-03 3.18181818e-02 3.24074074e-02]\n",
      " [4.86991438e-05 1.15302257e-03 0.00000000e+00]\n",
      " [2.44532539e-02 3.22580645e-02 0.00000000e+00]\n",
      " [2.12469651e-02 5.23577027e-02 0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.24704623, 0.44271949, 0.31023428])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_linear_interpolation.optimize(devData, 800, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the model with the optimal lambdas and we expect the perplexity againt the dev datset to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 12.898964375272874\n",
      "The perplexity of this model against the dev dataset is: 120.72354673770525\n",
      "The perplexity of this model against the test dataset is: 119.75103728465692\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.24704623, 0.44271949, 0.31023428])\n",
    "print(\"The perplexity of this model against the training dataset is:\", trigram_linear_interpolation.perplexity(sentences))\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))\n",
    "print(\"The perplexity of this model against the test dataset is:\", trigram_linear_interpolation.perplexity(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgTcTlLuloHu"
   },
   "source": [
    "## 3 Preposition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jb0OQ-yltc3"
   },
   "outputs": [],
   "source": [
    "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
    "!wget -O dev.out https://github.com/qtli/COMP7607-Fall2023/blob/master/assignments/A1/data/prep/dev.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/prep/dev.in\", 'r') as rawFile:\n",
    "    devData = [line.strip() for line in rawFile]\n",
    "    \n",
    "with open(\"data/prep/dev.out\", 'r') as ansFile:\n",
    "    devAns = [line.strip() for line in ansFile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_PREP = \"<PREP>\"\n",
    "\n",
    "\n",
    "class PREP_PREDICTOR(LM_LINEAR_INTERPOLATION):\n",
    "    _ALL_PREP_TOKENS = [\"for\", \"in\", \"of\", \"at\", \"on\"]\n",
    "    \n",
    "    def __init__(self, trainData, n):\n",
    "        super().__init__(trainData, n)\n",
    "        \n",
    "        \n",
    "    def _preprocess(self, rawData, ansData):\n",
    "        \"\"\"preprocess datainput by combining the trainData and the ansData\n",
    "        Args:\n",
    "            rawData: the training dataset with <PREP> masked\n",
    "            ansData: the answers in corresponding masks for each trainData\n",
    "        Returns:\n",
    "            list: a list of data combined with answers\n",
    "        \"\"\"\n",
    "        combinedData = []\n",
    "        n = len(rawData)\n",
    "        for i in range(n):\n",
    "            data = rawData[i]\n",
    "            ansTokens = ansData[i].split(\" \")\n",
    "            for ans in ansTokens:\n",
    "                data = data.replace(G_PREP, ans, 1)\n",
    "            combinedData.append(data)\n",
    "        return combinedData\n",
    "    \n",
    "    def perplexity(self, testData, testAns):\n",
    "        combinedData, _ = self._preprocess(testData, testAns)\n",
    "        return super().perplexity(combinedData)\n",
    "    \n",
    "    def optimize(self, data, ans, n_iter, learning_rate):\n",
    "        combinedData = self._preprocess(data, ans)\n",
    "        return super().optimize(combinedData, n_iter, learning_rate)\n",
    "    \n",
    "    def predictOne(self, sentTokens):\n",
    "        \"\"\"given a list of tokens, predict all preps\n",
    "\n",
    "        Args:\n",
    "            data: string sentence.\n",
    "        \"\"\"\n",
    "        resultPrep = []\n",
    "        resultProb = []\n",
    "        # startidx for searching <PREP> parttern\n",
    "        startIdx = 0\n",
    "        while(1):\n",
    "            # print(\"---------\")\n",
    "            try:\n",
    "                index = sentTokens.index(G_PREP, startIdx)\n",
    "                startIdx = index + 1\n",
    "                # n-1gram without the GPREP \n",
    "                n_1gram = tuple(sentTokens[index - self._N + 1: index])\n",
    "                # find the best prep and the associated probability\n",
    "                prepToken, prob = self._bestPREP(n_1gram)\n",
    "                sentTokens[index] = prepToken\n",
    "                resultPrep.append(prepToken)\n",
    "                resultProb.append(prob)\n",
    "            except ValueError:\n",
    "                break\n",
    "        return resultPrep, resultProb\n",
    "    \n",
    "    def predictData(self, data, path):\n",
    "        \"\"\"Predict all preps in a dataset of sentences, and write result to a file.\n",
    "        Args:\n",
    "            data: a list of sentences containing <PREP> masks.\n",
    "            path: the path of the file to store the result\n",
    "        \"\"\"\n",
    "        with open(path, 'w') as file:\n",
    "            for sentence in data:\n",
    "                sentTokens = self._tokenize([sentence])\n",
    "                predicts, probs = self.predictOne(sentTokens)\n",
    "                self._writeListToFile(predicts, file)\n",
    "    \n",
    "    def performace(self, data, ans):\n",
    "        \"\"\"Evaluate the performace on a dataset\n",
    "        Args:\n",
    "            data: a list of sentences masked by <PREP> to be predicted\n",
    "            ans: a list of corresponding answers\n",
    "        Return: rate of correctness\n",
    "        \"\"\"\n",
    "        correctCount = 0\n",
    "        totalCount = 0\n",
    "        for i in range(len(data)):\n",
    "            sentTokens = self._tokenize([data[i]])\n",
    "            predicts, probs = self.predictOne(sentTokens)\n",
    "            actuals = ans[i].split(\" \")\n",
    "            for a, b in zip(predicts, actuals):\n",
    "                totalCount += 1\n",
    "                if (a == b):\n",
    "                    correctCount += 1\n",
    "        return correctCount / totalCount\n",
    "            \n",
    "    \n",
    "    def _writeListToFile(self, lst, file):\n",
    "        \"\"\"Helper function to write a list to a file\n",
    "        \"\"\"\n",
    "        file.write(lst[0])\n",
    "        for item in lst[1:]:\n",
    "            file.write(\" \" + item)\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "    \n",
    "    def _bestPREP(self, n_1gram):\n",
    "        \"\"\"Given a n-1gram predict the next preposition by selecting the prep with the highest probability\n",
    "        \n",
    "        Agrs:\n",
    "            n_1gram: n-1 gram.\n",
    "        \"\"\"\n",
    "        bestPrep = None\n",
    "        bestProb = 0\n",
    "        for prepToken in self._ALL_PREP_TOKENS:\n",
    "            prob = self._calcProb(n_1gram + (prepToken,))\n",
    "            if prob > bestProb:\n",
    "                bestProb = prob\n",
    "                bestPrep = prepToken\n",
    "        return bestPrep, bestProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1gram model...\n",
      "training 2gram model...\n",
      "training 3gram model...\n",
      "Have a look at the submodels:\n",
      "[('<s>',): 0.039273503031521696] [('facebook',): 0.0005435452819562603] \n",
      "[('<s>', 'facebook'): 0.00538] [('facebook', 'has'): 0.11271676300578035] \n",
      "[('<s>', '<s>', 'facebook'): 0.00538] [('<s>', 'facebook', 'has'): 0.24907063197026022] \n"
     ]
    }
   ],
   "source": [
    "predictor = PREP_PREDICTOR(trainData, 3)\n",
    "predictor.train()\n",
    "print(\"Have a look at the submodels:\")\n",
    "predictor.peekModel(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glimpse at the probability matrix of the data optimized upon...\n",
      "[[2.16789737e-04 5.60000000e-04 5.60000000e-04 5.60000000e-04\n",
      "  5.60000000e-04]\n",
      " [3.77025629e-04 6.52173913e-02 2.50000000e-01 2.50000000e-01\n",
      "  2.50000000e-01]\n",
      " [6.04811947e-05 7.08333333e-02 8.88888889e-01 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [6.28376049e-05 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [1.04286860e-02 3.75000000e-02 3.89610390e-02 2.94117647e-02\n",
      "  0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.24586591, 0.42485175, 0.20751483, 0.04525108, 0.07651644])"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.optimize(devData, devAns, 1000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.updateLambdas([0.24586591, 0.42485175, 0.20751483, 0.04525108, 0.07651644])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predictData(devData, \"data/prep/test.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5883590462833099"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.performace(devData, devAns)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
