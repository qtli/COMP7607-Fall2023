{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEd-mtF0PTrv"
   },
   "source": [
    "# Assignment 1\n",
    "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8c9YBduQCI4"
   },
   "source": [
    "## 1 $n$-gram Language Model\n",
    "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) \\stackrel{\\tiny{\\mbox{def}}}{=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMQ_Z1g8QZef"
   },
   "source": [
    "**Write your answer:**\n",
    "\n",
    "\\begin{align}\n",
    "p(\\vec{w}) &= \\frac{C(w_1)}{C(W)} \\cdot \\frac{C(w_1w_2)}{C(w_1)} \\cdot \\frac{C(w_1w_2w_3)}{C(w_1w_2)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\dots \\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "&= \\frac{1}{C(W)} \\cdot \\frac{1}{1} \\cdot \\frac{C{w_1w_2w_3}}{1} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\dots \\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "&= \\frac{C{w_1w_2w_3}}{C(W)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\dots \\frac{C(w_{n-2}w_{n-1}w_n)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmgExbf1QtCH"
   },
   "source": [
    "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
    "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
    "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm1ZGFIaRPCP"
   },
   "source": [
    "**Write your answer:**\n",
    "\n",
    "\\begin{align}\n",
    "p_{reversed}(\\vec{w}) &=\\frac{C(w_1w_2w_3)}{C(w_2w_3)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_3w_4)} \\dots \\frac{C(w_{n-3}w_{n-2}w_{n-1})}{C(w_{n-2}w_{n-1})} \\cdot \\frac{C(w_{n-2}w_{n-1}w)}{C({w_{n-1}w_n})} \\cdot \\frac{C(w_{n-1}w_n)}{C(w_n)} \\cdot \\frac{C(w_n)}{C(W)} \\\\\n",
    "&= \\frac{C(w_1w_2w_3)}{1} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\cdot \\frac{C(w_3w_4w_5)}{C(w_3w_4)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\cdot \\frac{C(w_{n-1}w_n)}{C(w_{n-1}w_n)} \\cdot \\frac{C(w_n)}{C(w_n)} \\cdot \\frac{1}{C(W)} \\\\\n",
    "&= \\frac{C(w_1w_2w_3)}{C(W)} \\cdot \\frac{C(w_2w_3w_4)}{C(w_2w_3)} \\cdot \\frac{C(w_3w_4w_5)}{C(w_3w_4)} \\dots \\frac{C(w_{n-2}w_{n-1}w)}{C(w_{n-2}w_{n-1})} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Now, with some simplification, we can show that the formula of $p_{reversed}(\\vec{w})$ is the same as the one of $p(\\vec{w})$ answered in the previous question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQEc5kz4RniG"
   },
   "source": [
    "## 2 $N$-gram Language Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3kSwtN79jWgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-23 00:37:14--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6640478 (6.3M) [text/plain]\n",
      "Saving to: 'train.txt'\n",
      "\n",
      "train.txt           100%[===================>]   6.33M  12.6MB/s    in 0.5s    \n",
      "\n",
      "2023-10-23 00:37:15 (12.6 MB/s) - 'train.txt' saved [6640478/6640478]\n",
      "\n",
      "--2023-10-23 00:37:15--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 872910 (852K) [text/plain]\n",
      "Saving to: 'dev.txt'\n",
      "\n",
      "dev.txt             100%[===================>] 852.45K  3.45MB/s    in 0.2s    \n",
      "\n",
      "2023-10-23 00:37:16 (3.45 MB/s) - 'dev.txt' saved [872910/872910]\n",
      "\n",
      "--2023-10-23 00:37:16--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 869318 (849K) [text/plain]\n",
      "Saving to: 'test.txt'\n",
      "\n",
      "test.txt            100%[===================>] 848.94K  2.34MB/s    in 0.4s    \n",
      "\n",
      "2023-10-23 00:37:17 (2.34 MB/s) - 'test.txt' saved [869318/869318]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
    "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
    "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9HCVQwqkTc_"
   },
   "source": [
    "### 2.1 Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "# threshold of frequency to be classified as <UNK>\n",
    "G_FREQ_THRE = 3\n",
    "G_UNK = \"<UNK>\"\n",
    "G_SOS = \"<s>\"\n",
    "G_EOS = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "G_N = 1\n",
    "G_WORD_TOKENS = []\n",
    "G_VOCAB_TABLE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-KhFKCzwkaTn"
   },
   "outputs": [],
   "source": [
    "with open(\"train.txt\", 'r') as trainFile:\n",
    "    # sentences = [\"<s> \" + line.strip() + \" </s>\" for line in trainFile]\n",
    "    trainData = [line.strip() for line in trainFile]\n",
    "    \n",
    "with open(\"dev.txt\", 'r') as devFile:\n",
    "    devData = [line.strip() for line in devFile]\n",
    "    \n",
    "with open(\"test.txt\", 'r') as testFile:\n",
    "    testData = [line.strip() for line in testFile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training datset, and get sentence tokenization out of the training dataset, also add starting/end symbols for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCustomWordTokens(sentence):\n",
    "    pattern = r'<\\w+>|[#$\"\\(\\)\\',.!-]|\\d+(?:,\\d{3})*(?:\\.\\d+)?|\\w+(?:-\\w+)*|\\S+'\n",
    "    G_VOCAB_TABLE\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    wordTokens = tokenizer.tokenize(sentence)\n",
    "    return wordTokens\n",
    "\n",
    "def getNLTKWordTokens(sentence):\n",
    "    wordTokens = word_tokenize(sentence)\n",
    "    return wordTokens\n",
    "\n",
    "def tokenize(sentences):\n",
    "    word_tokens = []\n",
    "    for sentence in sentences:\n",
    "        # add SOS\n",
    "        result = [G_SOS] * max(1, (G_N - 1))\n",
    "        # tokenization\n",
    "        tokens = getCustomWordTokens(sentence)\n",
    "        # concatenate tokens and EOS\n",
    "        result.extend(tokens)\n",
    "        result.append(G_EOS)\n",
    "        # formulation\n",
    "        word_tokens.extend(result)\n",
    "    return word_tokens\n",
    "        \n",
    "def initLM(n = 1):\n",
    "    global G_TOKENIZED_SENTS, G_WORD_TOKENS, G_VOCAB_TABLE, G_N \n",
    "    G_TOKENIZED_SENTS = []\n",
    "    G_WORD_TOKENS = []\n",
    "    G_VOCAB_TABLE = {}\n",
    "    G_N = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokenization\n",
    "initLM()\n",
    "G_WORD_TOKENS = tokenize(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1273123 tokens in the raw training dataset.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(G_WORD_TOKENS), \"tokens in the raw training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the training dataset tokenized, we can build out vocabulary by creating a frequency table out of the tokens, and convert tokens appeared less than 3 times to `<UNK>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov(tokens):    \n",
    "    # create a vocab that contains frequency information of each token\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    # replace tokens with UNK if their freq is too low\n",
    "    return [token if vocab[token] >= G_FREQ_THRE else G_UNK for token in tokens]\n",
    "    \n",
    "G_WORD_TOKENS = oov(G_WORD_TOKENS)\n",
    "G_VOCAB_TABLE = nltk.FreqDist(G_WORD_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final vocabulary size is 17552\n",
      "<s>: 50000\n",
      "facebook: 692\n",
      "has: 6071\n",
      "released: 529\n",
      "a: 31132\n",
      "report: 312\n",
      "that: 10032\n",
      "shows: 289\n",
      "what: 607\n",
      "content: 377\n"
     ]
    }
   ],
   "source": [
    "print(\"The final vocabulary size is\", len(G_VOCAB_TABLE.keys()))\n",
    "\n",
    "demo = list(G_VOCAB_TABLE.items())[:10]\n",
    "for key, val in demo:\n",
    "    print(f\"{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLQNsUA5kfZe"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "The vocabulary size is `17552` after inserting `SOS/EOS` and replacing less frequent tokens with `UNK`. \n",
    "\n",
    "For a ngram model, we're building probability table which contains all combinations of length n given the vocabulary. Therefore, the number of parameters will be $|V|^n$. This is going to be enormous as vocabulary size and `n` increase. Furthermore, the probability table is going to be more and more **sparse** as `n` increases. Hence, we can optimize the probability table by storing only the ngrams that appeared in the training dataset, and provide a default value for unseen data. This can reduce the parameters to store down to `|V|`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJzDNMVikkeX"
   },
   "source": [
    "### 2.2 $N$-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxkcs2HykuR2"
   },
   "source": [
    "To genralize the process of building n-gram models, we'll create a class that contains all the functions we used in previous section as class methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDict(dic, n):\n",
    "    for key, val in dic.items():\n",
    "        if n == 0:\n",
    "            break\n",
    "        print(f\"[{key}: {val}]\", end=\" \")\n",
    "        n -= 1\n",
    "\n",
    "class LM:\n",
    "    # parameters\n",
    "    _N = 1\n",
    "    # all individual word tokens\n",
    "    _word_tokens = []\n",
    "    # a dict that stores all the frequence of indivicual tokens\n",
    "    _vocab = {}\n",
    "    \n",
    "    _ngrams = []\n",
    "    # a probability table of all ngrams\n",
    "    _model = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    def __init__(self, train_sentences, n = 1):\n",
    "        self._N = max(1, n)\n",
    "        # tokenize a list of sentences into a single giant list of tokens with SOS/EOS symbols.\n",
    "        tokens = self._tokenize(train_sentences)\n",
    "        # update with UNK\n",
    "        tokens = self._oov(tokens)\n",
    "        self._word_tokens = tokens\n",
    "        self._vocab = nltk.FreqDist(self._word_tokens)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Create ngram model without doing any smoothing.\n",
    "        \n",
    "        This method purely calculates the probability of a given ngram based on\n",
    "        the frequence of the given ngram and the frequence of its predecessor.\n",
    "        \n",
    "        Returns:\n",
    "            dict: the probability table of ngrams.\n",
    "        \"\"\"\n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        \n",
    "\n",
    "        # create grams with window size n - 1\n",
    "        n_1_grams = nltk.ngrams(self._word_tokens, self._N-1)\n",
    "        n_1_vocab = nltk.FreqDist(n_1_grams)\n",
    "\n",
    "        # for unigram\n",
    "        if self._N == 1:\n",
    "            tokenSize = len(self._word_tokens)\n",
    "            self._model = {token: freq / tokenSize for token, freq in n_vocab.items()}\n",
    "        else:\n",
    "            # create the probability table for ngram\n",
    "            self._model = {ngram: freq / n_1_vocab[ngram[:-1]] for ngram, freq in n_vocab.items()}\n",
    "    \n",
    "    def getModel(self):\n",
    "        return self._model\n",
    "        \n",
    "            \n",
    "    def perplexity(self, testData):\n",
    "        \"\"\"Calculating perplexity given a list of sentences.\n",
    "        \n",
    "        Args:\n",
    "            testTokens: a list of tokens\n",
    "        Returns:\n",
    "            float: the perplexity\n",
    "        \"\"\"\n",
    "        # toknization\n",
    "        testTokens = self._tokenize(testData)\n",
    "        return self._perplexity(testTokens)\n",
    "\n",
    "\n",
    "    def _perplexity(self, testTokens):\n",
    "        \"\"\"A private version of calculating perplexity given a list tokens.\n",
    "        \n",
    "        Args:\n",
    "            testTokens: a list of tokens\n",
    "        Returns:\n",
    "            float: the perplexity\n",
    "        \"\"\"\n",
    "        # replace oov with UNK\n",
    "        kownVocab = self._vocab.keys()\n",
    "        testTokens = [token if token in kownVocab else G_UNK for token in testTokens]\n",
    "        # get test grams to calculate probabilities\n",
    "        testGrams = list(nltk.ngrams(testTokens, self._N))\n",
    "        \n",
    "        logs = [math.log(self._calcProb(ngram)) for ngram in testGrams]\n",
    "        return math.exp((-1 / len(testTokens)) * sum(logs))\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _calcProb(self, ngram):\n",
    "        \"\"\"Calculate the probability of a given ngram based on current model.\n",
    "\n",
    "        For unseen ngrams, the probability is 0.\n",
    "\n",
    "        Args:\n",
    "            ngram: the traget.\n",
    "        Returns:\n",
    "            float: the probability\n",
    "        \"\"\"\n",
    "        # check if the ngram is known\n",
    "        return self._model.get(ngram, 0)\n",
    "    \n",
    "    def peekTokens(self, n):\n",
    "        \"\"\"Have a peek at the tokens\n",
    "        \n",
    "        Args:\n",
    "            n: the number of elements to print.\n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "            print(self._word_tokens[i], end=\" \")\n",
    "    \n",
    "    def peekVocab(self, n):\n",
    "        printDict(self._vocab, n)\n",
    "            \n",
    "    def getVocabSize(self):\n",
    "        return len(sel._vocab)\n",
    "    \n",
    "    def peekModel(self, n):\n",
    "        \"\"\"Have a peek at the ngram table created\n",
    "        \n",
    "        Args:\n",
    "            n: the number of elements to print.\n",
    "        \"\"\"\n",
    "        printDict(self._model, n)\n",
    "        \n",
    "    def _tokenize(self, sentences):\n",
    "        \"\"\"Transform a list of sentences into a list of tokens.\n",
    "        \n",
    "        This function noly only tokenize the input, but also add SOS and EOS symbol before\n",
    "        and after each sentence.\n",
    "        \n",
    "        Args:\n",
    "            sentences: A list of sentences: str, expected to be the training data.\n",
    "            \n",
    "        Returns:\n",
    "            list: A single giant list of tokens that appeared in the input, with the same order\n",
    "                  as the input.\n",
    "        \"\"\"\n",
    "        word_tokens = []\n",
    "        for sentence in sentences:\n",
    "            # add SOS\n",
    "            result = [G_SOS] * max(1, (self._N - 1))\n",
    "            # tokenization\n",
    "            tokens = self._getCustomWordTokens(sentence)\n",
    "            # concatenate tokens and EOS\n",
    "            result.extend(tokens)\n",
    "            result.append(G_EOS)\n",
    "            # formulation\n",
    "            word_tokens.extend(result)\n",
    "        return word_tokens\n",
    "    \n",
    "        \n",
    "        \n",
    "    def _oov(self, tokens):\n",
    "        \"\"\"This function replace any tokens with low frequence of appearence with UNK\n",
    "        \n",
    "        Args:\n",
    "            tokens: word tokens\n",
    "        \"\"\"\n",
    "        # create a vocab that contains frequnce information of each token\n",
    "        vocab = nltk.FreqDist(tokens)\n",
    "        # replae tokens with UNK if their freq is too low\n",
    "        return [token if vocab[token] >= G_FREQ_THRE else G_UNK for token in tokens]\n",
    "    \n",
    "    def _getCustomWordTokens(self, sentence):\n",
    "        pattern = r'<\\w+>|[#$\"\\(\\)\\',.!-]|\\d+(?:,\\d{3})*(?:\\.\\d+)?|\\w+(?:-\\w+)*|\\S+'\n",
    "        G_VOCAB_TABLE\n",
    "        tokenizer = RegexpTokenizer(pattern)\n",
    "        wordTokens = tokenizer.tokenize(sentence)\n",
    "        return wordTokens\n",
    "\n",
    "    def _getNLTKWordTokens(self, sentence):\n",
    "        wordTokens = word_tokenize(sentence)\n",
    "        return wordTokens\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By introducing `_calProb(ngram)` function, which checks the probability table for a given ngram and return its corresponding probability or `0` if the ngram is unseen, we can store only ngrams we've seen. So the size of parameters can be shrink down to $|V|$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',): 0.039273503031521696] [('facebook',): 0.0005435452819562603] [('has',): 0.0047685887380873645] [('released',): 0.0004155136620734996] [('a',): 0.02445325392754667] [('report',): 0.0002450666589166954] [('that',): 0.007879835648244514] [('shows',): 0.00022700084752219542] [('what',): 0.0004767803268026734] [('content',): 0.0002961222128576736] "
     ]
    }
   ],
   "source": [
    "unigram = LM(trainData)\n",
    "unigram.train()\n",
    "unigram.peekModel(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model created without using any smoothing technics, let's try to analyze its perplxity againt the training data and dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Unigram model against the training dataset is: 796.9548612984283\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this Unigram model against the training dataset is:\", unigram.perplexity(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Unigram against the dev dataset is: 803.0081866449137\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this Unigram against the dev dataset is:\", unigram.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'facebook'): 0.00538] [('facebook', 'has'): 0.11271676300578035] [('has', 'released'): 0.015977598418711908] [('released', 'a'): 0.1720226843100189] [('a', 'report'): 0.003950918668893743] [('report', 'that'): 0.041666666666666664] [('that', 'shows'): 0.006379585326953748] [('shows', 'what'): 0.03460207612456748] [('what', 'content'): 0.0016474464579901153] [('content', 'was'): 0.007957559681697613] "
     ]
    }
   ],
   "source": [
    "bigram = LM(trainData, 2)\n",
    "bigram.train()\n",
    "bigram.peekModel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Bigram model against the training dataset is: 48.442156606470306\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this Bigram model against the training dataset is:\", bigram.perplexity(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe perplexity of this Bigram model against the dev dataset is:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mbigram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevData\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 71\u001b[0m, in \u001b[0;36mLM.perplexity\u001b[0;34m(self, testData)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# get test grams to calculate probabilities\u001b[39;00m\n\u001b[1;32m     69\u001b[0m testGrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mngrams(testTokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N))\n\u001b[0;32m---> 71\u001b[0m logs \u001b[38;5;241m=\u001b[39m [math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calcProb(ngram)) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m testGrams]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mexp((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(testTokens)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(logs))\n",
      "Cell \u001b[0;32mIn[12], line 71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# get test grams to calculate probabilities\u001b[39;00m\n\u001b[1;32m     69\u001b[0m testGrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nltk\u001b[38;5;241m.\u001b[39mngrams(testTokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N))\n\u001b[0;32m---> 71\u001b[0m logs \u001b[38;5;241m=\u001b[39m [\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calcProb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m testGrams]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mexp((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(testTokens)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(logs))\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this Bigram model against the dev dataset is:\", bigram.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3o9Nez8kvYm"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "By running the perplexity analysis against the dev dataset, **there's an error occurred** complaining the math domain error. This error demonstrates a problem of our implementation of creating ngrams probability table. **That is, it can't handle unseen data**. When an unseen ngram occurs in the test dataset the probability of it will be `0`. In this implementation, we use log probability during calculating the perplexity to reduce floating number overflow when keep multiplying probabilities. Hence, during calculating the perplexity, we'll have `log(0)` in the equation, which is an undefined behavior. This issue can be resolved by smoothing if we 'donate' some probability mass from seen ngrams to unseen ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOQUqM73kzf-"
   },
   "source": [
    "### 2.3 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LgXRmJwk3Y-"
   },
   "source": [
    "#### 2.3.1 Add-one (Laplace) smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_ADD_K(LM):\n",
    "    _K = 1\n",
    "    # model related\n",
    "    _subVocab = {}\n",
    "    def __init__(self, sentences, n=1):\n",
    "        super().__init__(sentences, n)\n",
    "    \n",
    "    def train(self, k = 1):\n",
    "        self._K = k\n",
    "        self._smoothing()\n",
    "        \n",
    "    def _smoothing(self):\n",
    "        \"\"\"Create a ngram model with add_k_smoothing.\n",
    "        \n",
    "        When k is 1, this function creates model with laplace smoothing.\n",
    "        \n",
    "        Agrs:\n",
    "            k: the increment amount for freq of each ngrams.\n",
    "        \"\"\"\n",
    "        # unigram doesn't support smoothing\n",
    "        if self._N == 1:\n",
    "            print(\"unigram model doesn't support smoothing, continue without smoothing...\")\n",
    "            return super().train()\n",
    "        \n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        \n",
    "        # create grams with window size n - 1\n",
    "        n_1_grams = nltk.ngrams(self._word_tokens, self._N-1)\n",
    "        n_1_vocab = nltk.FreqDist(n_1_grams)\n",
    "        self._subVocab = n_1_vocab\n",
    "        \n",
    "        vocabSize = len(self._vocab)\n",
    "        self._model = {ngram: (freq + self._K) / (n_1_vocab[ngram[:-1]] + self._K * vocabSize) for ngram, freq in n_vocab.items()}        \n",
    "    \n",
    "    \n",
    "    def _calcProb(self, ngram):\n",
    "        \"\"\"Calculate the probability of a given ngram based on current model\n",
    "        \n",
    "        When ngram is known, directly return the probability\n",
    "        When ngram is unkown and smoothing is not used, return 0\n",
    "        When ngram is unknown but smoothing is used, return smoothed value.\n",
    "        \n",
    "        Args:\n",
    "            ngram: the traget.\n",
    "        \"\"\"\n",
    "        # check if the ngram is known         \n",
    "        prob = self._model.get(ngram, 0)\n",
    "        \n",
    "        if prob == 0:    \n",
    "            vocabSize = len(self._vocab)\n",
    "            subFreq = self._subVocab.get(ngram[:-1], 0)\n",
    "            return self._K / (subFreq + self._K * vocabSize)\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lFG7jCIRk7Qw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'facebook'): 0.003996920890573188] [('facebook', 'has'): 0.0043301907476430606] [('has', 'released'): 0.004148499343859797] [('released', 'a'): 0.005088214147447597] [('a', 'report'): 0.002547038041245584] [('report', 'that'): 0.0007836990595611285] [('that', 'shows'): 0.0023564385150812066] [('shows', 'what'): 0.000616557367860546] [('what', 'content'): 0.00011013822347045542] [('content', 'was'): 0.00022310223659992193] "
     ]
    }
   ],
   "source": [
    "bigram_laplace = LM_ADD_K(trainData, 2)\n",
    "bigram_laplace.train()\n",
    "bigram_laplace.peekModel(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Bigram with Laplace smoothing against the training dataset is: 678.3425847140647\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this Bigram model with Laplace smoothing against the training dataset is:\", bigram_laplace.perplexity(trainData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Bigram model with Laplace against the dev dataset is: 829.2794789075276\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this Bigram model with Laplace against the dev dataset is:\", bigram_laplace.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36yTKPXFk8f2"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "With smoothing where some probability mass is distributed to all combinations of ngrams including unseen ones, we no longer have the issue discussed in last section. Therefore, we can observe the perplexity of the model against the dev dataset is `829.27`. We also use `_calProb()` technique introduced previously to reduce number of parameters by introducing an additional layer of logic.\n",
    "\n",
    "However, the perplexity of the new model against the training dataset is much higher than the previous model without smoothing. This case happens because the **training dataset contains only ngrams the model knows**, meaning it expects the probability mass on ngrams in the training dataset to be as much as possible. However, after smoothing, some probability mass is **donated** to unseen ngrams, which is not helping in this case as we'll never encounter unseen ngrams in the training dataset and those mass are sort of **wasted**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8cFbczqlBR_"
   },
   "source": [
    "#### 2.3.2: Add-$k$ smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uV_ZiAgIlPUu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Bigram model with 0.5 k smoothing against the training dataset is: 459.10639016556837\n",
      "The perplexity of this Bigram model with 0.5 k smoothing against the dev dataset is: 608.8950847554092\n"
     ]
    }
   ],
   "source": [
    "bigram_add_k = LM_ADD_K(trainData, 2)\n",
    "bigram_add_k.train(0.5)\n",
    "print(\"The perplexity of this Bigram model with 0.5 k smoothing against the training dataset is:\", bigram_add_k.perplexity(trainData))\n",
    "print(\"The perplexity of this Bigram model with 0.5 k smoothing against the dev dataset is:\", bigram_add_k.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Bigram model with 0.1 k smoothing against the training dataset is: 199.41218240875557\n",
      "The perplexity of this Bigram model with 0.1 k smoothing against the dev dataset is: 338.2739273653482\n"
     ]
    }
   ],
   "source": [
    "bigram_add_k.train(0.1)\n",
    "print(\"The perplexity of this Bigram model with 0.1 k smoothing against the training dataset is:\", bigram_add_k.perplexity(trainData))\n",
    "print(\"The perplexity of this Bigram model with 0.1 k smoothing against the dev dataset is:\", bigram_add_k.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this Bigram model with 0.001 k smoothing against the training dataset is: 55.751861780817016\n",
      "The perplexity of this Bigram model with 0.001 k smoothing against the dev dataset is: 217.6393120902775\n"
     ]
    }
   ],
   "source": [
    "bigram_add_k.train(0.001)\n",
    "print(\"The perplexity of this Bigram model with 0.001 k smoothing against the training dataset is:\", bigram_add_k.perplexity(trainData))\n",
    "print(\"The perplexity of this Bigram model with 0.001 k smoothing against the dev dataset is:\", bigram_add_k.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHFNf8OIlQ0O"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "Using add-k-smoothing, the perplexity of the model is less than the one with laplace. Especially, when k is getting lower. The reason a lower k generates better result can be the followings. Given a unigram, if the size of its corresponding bigram is limited, after smoothing, the probability of the original seen bigram will be spreaded drastically. For example, For bigrams $a[B]$ (all bigrams starting with letter `a`), if the size of $B$ is 3. For each seen bigram, their probability is $0.333$. After smoothing which spread the probability mass to the additional $|V| - 3$ number of bigrams. Their probability will reduced from $0.333$ to a significant small amount, and the differences between their probability and unseen onces are negligible. Therefore, by lowering the factor `k`, we can reduce the amount of probability mass spread to unseen data, hence remain the dominance of the seen bigrams in the probability mass. On the other hand, observing that keep lowering the factor `k`, the perplexity againt the dev dataset is also reducing, we can also infer that the dev dataset is similar to the training dataset. Otherwise, giving more probability mass to unseen data should produce better result for dev dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjKEO_TqlUrX"
   },
   "source": [
    "#### 2.3.3 Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pcdd4cvYlZuO"
   },
   "outputs": [],
   "source": [
    "class LM_LINEAR_INTERPOLATION(LM):\n",
    "    _LAMBDAS = []\n",
    "    _PROB_BI = {}\n",
    "    _PROB_UNI = {}\n",
    "    _RAW_DATA = None\n",
    "    def __init__(self, sentences, n):\n",
    "        super().__init__(sentences, n)\n",
    "        self._RAW_DATA = sentences\n",
    "        self._model = []\n",
    "        self._LAMBDAS = [1/self._N]*self._N\n",
    "        \n",
    "    \n",
    "    def trainWithUniformWordTokens(self):\n",
    "        \"\"\"\n",
    "        All submodels in this training function will use the same wordTokens which is generated by the highest order model.\n",
    "        The potential problem is that SOS/EOS are introduced multiple times in the highest order model. EOS/SOS tokens will dominate\n",
    "        in the lower oder models. \n",
    "        Args:\n",
    "            ls: a list of lambdas in sequential order L0...LN, if not provided, automatically generated with equal weights.\n",
    "        \"\"\"\n",
    "        if self._N == 1:\n",
    "            print(\"unigram model doesn't support smoothing, continue without smoothing...\")\n",
    "            return super().train()\n",
    "        super().__init__(self._RAW_DATA, self._N)\n",
    "        \n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        for i in range(1, len(self._LAMBDAS)):\n",
    "            # create grams with window size n - i\n",
    "            n_1_grams = nltk.ngrams(self._word_tokens, self._N-i)\n",
    "            n_1_vocab = nltk.FreqDist(n_1_grams)\n",
    "            prob_n_gram = {ngram: (freq) / (n_1_vocab[ngram[:-1]]) for ngram, freq in n_vocab.items()}\n",
    "            self._model.append(prob_n_gram)\n",
    "            n_vocab = n_1_vocab\n",
    "        \n",
    "        tokenSize = len(self._word_tokens)\n",
    "        prob_uni_gram =  {token: freq / tokenSize for token, freq in n_vocab.items()}\n",
    "        self._model.append(prob_uni_gram)\n",
    "        # each time the probability is appened, so that higher-order models are in the front\n",
    "        # so reverse the _model\n",
    "        self._model.reverse()\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ls: a list of lambdas in sequential order L0...LN, if not provided, automatically generated with equal weights.\n",
    "        \"\"\"\n",
    "        if self._N == 1:\n",
    "            print(\"unigram model doesn't support smoothing, continue without smoothing...\")\n",
    "            return super().train()\n",
    "        self._model = []\n",
    "        \n",
    "        # create grams with window size n\n",
    "        n_grams = nltk.ngrams(self._word_tokens, self._N)\n",
    "        self._ngrams = list(n_grams)\n",
    "        n_vocab = nltk.FreqDist(self._ngrams)\n",
    "        for i in range(1, self._N + 1):\n",
    "            print(f\"training {i}gram model...\")\n",
    "            subModel = LM(self._RAW_DATA, i)\n",
    "            subModel.train()\n",
    "            self._model.append(subModel.getModel())\n",
    "        \n",
    "    def optimize(self, data, n_iter, learning_rate):\n",
    "        \"\"\"Optimize the lambda parameters using gradient descent.\n",
    "        \n",
    "        This method optimize by maximizing the probability of ngrams in the data, instead of minimizing the perpleixty.\n",
    "        This methods is explained in https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58.\n",
    "        \n",
    "        Agrs:\n",
    "            data: the data we wish to optimize the lambds upon\n",
    "            n_iter: the number of iterations to perform in finding the optimal lambdas.\n",
    "            learning_rate: the rate of descent for each iteration.\n",
    "        Returns:\n",
    "            list: a list of optimal lambdas for unigram ... ngram.\n",
    "        \"\"\"\n",
    "        # RESET LAMBDAS\n",
    "        self._LAMBDAS = [1/self._N]*self._N\n",
    "        \n",
    "        # toknization\n",
    "        optTokens = self._tokenize(data)\n",
    "        # replace oov with UNK\n",
    "        kownVocab = self._vocab.keys()\n",
    "        optTokens = [token if token in kownVocab else G_UNK for token in optTokens]\n",
    "        # get opt grams to calculate probabilities\n",
    "        optGrams = list(nltk.ngrams(optTokens, self._N))\n",
    "        \n",
    "        # get the probability matrix\n",
    "        probMatrix = self._probMatrix(optGrams)\n",
    "        \n",
    "        ngram_probs = probMatrix[:, 1:]\n",
    "        uniform_prob = probMatrix[:, [0]]\n",
    "        \n",
    "        weights = np.array(self._LAMBDAS)\n",
    "        \n",
    "        for iteration in range(n_iter):\n",
    "            # 2. Calculate gradients for each n-gram model\n",
    "            interpolated_probs = np.sum(probMatrix * weights, axis=1, keepdims=True)\n",
    "            ngram_gradients = np.mean((ngram_probs - uniform_prob) / interpolated_probs, axis=0)\n",
    "\n",
    "            # 3. Update interpolation weights for all models\n",
    "            weights[1:] += learning_rate * ngram_gradients\n",
    "            weights[0] = 1 - weights[1:].sum()\n",
    "        return weights\n",
    "    \n",
    "    def updateLambdas(self, ls):\n",
    "        if ls is None:\n",
    "            return\n",
    "        # if (sum(ls) != 1):\n",
    "        #     print(\"Incorrect lambdas, sum not equal to 1\")\n",
    "        #     return\n",
    "        if (len(ls) != self._N):\n",
    "            print(\"Incorrect lambda length\")\n",
    "            return\n",
    "        self._LAMBDAS = ls\n",
    "        \n",
    "    \n",
    "    def _probMatrix(self, ngrams):\n",
    "        # a probability matrix of grams x models\n",
    "        matrix = np.zeros((len(ngrams), self._N))\n",
    "        for i in range(len(self._model)):\n",
    "            matrix[:, [i]] = np.array([self._model[i].get(ngram[self._N - i - 1:], 0) for ngram in ngrams]).reshape(-1, 1)\n",
    "        print(\"A glimpse at the probability matrix of the data optimized upon...\")\n",
    "        print(matrix[:5, :5])\n",
    "        return matrix\n",
    "        \n",
    "    def _calcProb(self, ngram):\n",
    "        prob = 0\n",
    "        # len(self._model) = self._N\n",
    "        for i in range(self._N):\n",
    "            # model stores prob distributions from 0 to N.\n",
    "            prob += self._model[i].get(ngram[self._N - 1 - i:], 0) * self._LAMBDAS[i]\n",
    "        if (prob == 0):\n",
    "            print(\"calcProb error: not found\", ngram)\n",
    "        return prob\n",
    "    \n",
    "    def peekModel(self, n):\n",
    "        for probs in self._model:\n",
    "            printDict(probs, n)\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a trigram model smoothed by linear interpolation with equal weights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1gram model...\n",
      "training 2gram model...\n",
      "training 3gram model...\n",
      "Have a look at the submodels:\n",
      "[('<s>',): 0.039273503031521696] [('facebook',): 0.0005435452819562603] [('has',): 0.0047685887380873645] \n",
      "[('<s>', 'facebook'): 0.00538] [('facebook', 'has'): 0.11271676300578035] [('has', 'released'): 0.015977598418711908] \n",
      "[('<s>', '<s>', 'facebook'): 0.00538] [('<s>', 'facebook', 'has'): 0.24907063197026022] [('facebook', 'has', 'released'): 0.02564102564102564] \n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation = LM_LINEAR_INTERPOLATION(trainData, 3)\n",
    "trigram_linear_interpolation.train()\n",
    "print(\"Have a look at the submodels:\")\n",
    "trigram_linear_interpolation.peekModel(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this trigram model with linear interpolation against the training dataset is: 12.953025318001819\n",
      "The perplexity of this trigram model with linear interpolation against the dev dataset is: 123.6382902418981\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of this trigram model with linear interpolation against the training dataset is:\", trigram_linear_interpolation.perplexity(trainData))\n",
    "print(\"The perplexity of this trigram model with linear interpolation against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try manually to optimize the lambdas to perform best on dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 132.4205743334757\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.3, 0.2, 0.5])\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 122.83927845909868\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.2, 0.4, 0.4])\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the dev dataset is: 127.17716778785392\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.2, 0.7, 0.1])\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 trials, we decide to use $\\lambda_1 = 0.2, \\lambda_2 = 0.4, \\lambda_3 = 0.4$ for our manually optimized weights to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the test dataset is: 121.94002570853857\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.2, 0.4, 0.4])\n",
    "print(\"The perplexity of this model against the test dataset is:\", trigram_linear_interpolation.perplexity(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "After using linear interpolation, we can find the perplexity drastically reduced compared to add-k smoothing. When training a ngram model, we always wish to increase `n` as it will give us more history information in predicting. However, as `n` increases, the model will be hungry for data, and without enough data, the probability table of model will be sparse, leading worse performance than lower order models. Linear interpolation gives us a flexibility to take context advantages of higher order model and mitigate its sparsity problem with lower order models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzSbk2bClf3u",
    "tags": []
   },
   "source": [
    "##### **Optimization**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, optimize the lambdas using [Gradient Descent](https://medium.com/mti-technology/n-gram-language-models-b125b9b62e58). Gradient Descent is normally used for unconstrained optimization problems, and this time, we have a constraint that the sum of the weights should be `1`. To mitigate this, we'll set the weight of the unigram to be $1 - \\lambda_2 - \\lambda_3$. Then, the formula of the probability can be writen as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P &= \\frac{1}{N_{word}} \\sum_{word} log P(word) \\\\\n",
    "  &= \\frac{1}{N_{word}} \\sum_{word} log ((1 - \\lambda_2 - \\lambda_3)P(w) + (\\lambda_2 P(w | w_{-1})) + (\\lambda_3 P(w | w_{-1}w_{-2})))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To differentiate upon each lambda, we can have the formula for $\\lambda_j$ as the followings:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\delta P}{\\delta \\lambda_j} &= \\frac{1}{N_{word}} \\sum_{word} \\frac{1}{P(word)} \\frac{\\delta P(word)}{\\delta \\lambda_j} \\\\\n",
    "&= \\frac{1}{N_{word}} \\sum_{word} \\frac{1}{P(word)}(-P(w) + P(w | w_{\\dots}w_{j-1}) )\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glimpse at the probability matrix of the data optimized upon...\n",
      "[[1.72803413e-04 4.32000000e-03 4.32000000e-03]\n",
      " [4.76858874e-03 3.18181818e-02 3.24074074e-02]\n",
      " [4.86991438e-05 1.15302257e-03 0.00000000e+00]\n",
      " [2.44532539e-02 3.22580645e-02 0.00000000e+00]\n",
      " [2.12469651e-02 5.23577027e-02 0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.24704623, 0.44271949, 0.31023428])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_linear_interpolation.optimize(devData, 800, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the model with the optimal lambdas and we expect the perplexity againt the dev datset to be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of this model against the training dataset is: 12.898964375272874\n",
      "The perplexity of this model against the dev dataset is: 170.5045386731055\n",
      "The perplexity of this model against the test dataset is: 119.75103728465692\n"
     ]
    }
   ],
   "source": [
    "trigram_linear_interpolation.updateLambdas([0.24704623, 0.44271949, 0.31023428])\n",
    "print(\"The perplexity of this model against the training dataset is:\", trigram_linear_interpolation.perplexity(trainData))\n",
    "print(\"The perplexity of this model against the dev dataset is:\", trigram_linear_interpolation.perplexity(devData))\n",
    "print(\"The perplexity of this model against the test dataset is:\", trigram_linear_interpolation.perplexity(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgTcTlLuloHu"
   },
   "source": [
    "## 3 Preposition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "7jb0OQ-yltc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-23 01:15:48--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 210427 (205K) [text/plain]\n",
      "Saving to: 'dev.in'\n",
      "\n",
      "dev.in              100%[===================>] 205.50K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-10-23 01:15:49 (1.51 MB/s) - 'dev.in' saved [210427/210427]\n",
      "\n",
      "--2023-10-23 01:15:49--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.out\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10018 (9.8K) [text/plain]\n",
      "Saving to: 'dev.out'\n",
      "\n",
      "dev.out             100%[===================>]   9.78K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2023-10-23 01:15:50 (4.64 MB/s) - 'dev.out' saved [10018/10018]\n",
      "\n",
      "--2023-10-23 01:15:50--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/test.in\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68304 (67K) [text/plain]\n",
      "Saving to: 'test.in'\n",
      "\n",
      "test.in             100%[===================>]  66.70K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2023-10-23 01:15:51 (1005 KB/s) - 'test.in' saved [68304/68304]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
    "!wget -O dev.out https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.out\n",
    "\n",
    "!wget -O test.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/test.in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dev.in\", 'r') as rawFile:\n",
    "    devData = [line.strip() for line in rawFile]\n",
    "    \n",
    "with open(\"dev.out\", 'r') as ansFile:\n",
    "    devAns = [line.strip() for line in ansFile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_PREP = \"<PREP>\"\n",
    "\n",
    "\n",
    "class PREP_PREDICTOR(LM_LINEAR_INTERPOLATION):\n",
    "    _ALL_PREP_TOKENS = [\"for\", \"in\", \"of\", \"at\", \"on\"]\n",
    "    \n",
    "    def __init__(self, trainData, n):\n",
    "        super().__init__(trainData, n)\n",
    "        \n",
    "        \n",
    "    def _preprocess(self, rawData, ansData):\n",
    "        \"\"\"preprocess datainput by combining the trainData and the ansData\n",
    "        Args:\n",
    "            rawData: the training dataset with <PREP> masked\n",
    "            ansData: the answers in corresponding masks for each trainData\n",
    "        Returns:\n",
    "            list: a list of data combined with answers\n",
    "        \"\"\"\n",
    "        combinedData = []\n",
    "        n = len(rawData)\n",
    "        for i in range(n):\n",
    "            data = rawData[i]\n",
    "            ansTokens = ansData[i].split(\" \")\n",
    "            for ans in ansTokens:\n",
    "                data = data.replace(G_PREP, ans, 1)\n",
    "            combinedData.append(data)\n",
    "        return combinedData\n",
    "    \n",
    "    def perplexity(self, testData, testAns):\n",
    "        combinedData, _ = self._preprocess(testData, testAns)\n",
    "        return super().perplexity(combinedData)\n",
    "    \n",
    "    def optimize(self, data, ans, n_iter, learning_rate):\n",
    "        combinedData = self._preprocess(data, ans)\n",
    "        return super().optimize(combinedData, n_iter, learning_rate)\n",
    "    \n",
    "    def predictOne(self, sentTokens):\n",
    "        \"\"\"given a list of tokens, predict all preps\n",
    "\n",
    "        Args:\n",
    "            data: string sentence.\n",
    "        \"\"\"\n",
    "        resultPrep = []\n",
    "        # startidx for searching <PREP> pattern\n",
    "        startIdx = 0\n",
    "        while(1):\n",
    "            # print(\"---------\")\n",
    "            try:\n",
    "                index = sentTokens.index(G_PREP, startIdx)\n",
    "                startIdx = index + 1\n",
    "                # n-1gram without the GPREP \n",
    "                b4_n_1grams = sentTokens[index - self._N + 1: index]\n",
    "                # 2 grams after GPREP\n",
    "                af_2grams = sentTokens[index+1: index+3]\n",
    "                # find the best prep and the associated perplexity\n",
    "                prepToken, _ = self._bestPREP(b4_n_1grams, af_2grams)\n",
    "                sentTokens[index] = prepToken\n",
    "                resultPrep.append(prepToken)\n",
    "            except ValueError:\n",
    "                break\n",
    "        return resultPrep\n",
    "    \n",
    "    def predictData(self, data, path):\n",
    "        \"\"\"Predict all preps in a dataset of sentences, and write result to a file.\n",
    "        Args:\n",
    "            data: a list of sentences containing <PREP> masks.\n",
    "            path: the path of the file to store the result\n",
    "        \"\"\"\n",
    "        with open(path, 'w') as file:\n",
    "            for sentence in data:\n",
    "                sentTokens = self._tokenize([sentence])\n",
    "                predicts = self.predictOne(sentTokens)\n",
    "                self._writeListToFile(predicts, file)\n",
    "    \n",
    "    def performacePerplexity(self, data, ans):\n",
    "        \"\"\"Evaluate the performace on a dataset\n",
    "        Args:\n",
    "            data: a list of sentences masked by <PREP> to be predicted\n",
    "            ans: a list of corresponding answers\n",
    "        Return: rate of correctness\n",
    "        \"\"\"\n",
    "        correctCount = 0\n",
    "        totalCount = 0\n",
    "        for i in range(len(data)):\n",
    "            sentTokens = self._tokenize([data[i]])\n",
    "            predicts = self.predictOne(sentTokens)\n",
    "            actuals = ans[i].split(\" \")\n",
    "            for a, b in zip(predicts, actuals):\n",
    "                totalCount += 1\n",
    "                if (a == b):\n",
    "                    correctCount += 1\n",
    "        return correctCount / totalCount\n",
    "            \n",
    "    \n",
    "    def _writeListToFile(self, lst, file):\n",
    "        \"\"\"Helper function to write a list to a file\n",
    "        \"\"\"\n",
    "        file.write(lst[0])\n",
    "        for item in lst[1:]:\n",
    "            file.write(\" \" + item)\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "    \n",
    "    def _bestPREP(self, b4, af_2):\n",
    "        \"\"\"Given a n- tokens and 2 tokens after the target, return the best Prep with lowest perplexity.\n",
    "\n",
    "        This function concatenate the all previous grams relative to the N, and the following 2 grams, and compare the perplexity\n",
    "        \n",
    "        Agrs:\n",
    "            b4: n-1grams before the target\n",
    "            af_2: 2 grams after the target\n",
    "        \"\"\"\n",
    "        bestPrep = None\n",
    "        bestPPL = math.inf\n",
    "        for prepToken in self._ALL_PREP_TOKENS:\n",
    "            candidateTokens = b4 + [prepToken] + af_2\n",
    "            ppl = self._perplexity(candidateTokens)\n",
    "            if ppl < bestPPL:\n",
    "                bestPPL = ppl\n",
    "                bestPrep = prepToken\n",
    "        return bestPrep, bestPPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1gram model...\n",
      "training 2gram model...\n",
      "training 3gram model...\n",
      "Have a look at the submodels:\n",
      "[('<s>',): 0.039273503031521696] [('facebook',): 0.0005435452819562603] \n",
      "[('<s>', 'facebook'): 0.00538] [('facebook', 'has'): 0.11271676300578035] \n",
      "[('<s>', '<s>', 'facebook'): 0.00538] [('<s>', 'facebook', 'has'): 0.24907063197026022] \n"
     ]
    }
   ],
   "source": [
    "predictor = PREP_PREDICTOR(trainData, 3)\n",
    "predictor.train()\n",
    "print(\"Have a look at the submodels:\")\n",
    "predictor.peekModel(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A glimpse at the probability matrix of the data optimized upon...\n",
      "[[2.16789737e-04 5.60000000e-04 5.60000000e-04]\n",
      " [3.77025629e-04 6.52173913e-02 2.50000000e-01]\n",
      " [6.04811947e-05 7.08333333e-02 8.88888889e-01]\n",
      " [6.28376049e-05 1.00000000e+00 1.00000000e+00]\n",
      " [1.04286860e-02 3.75000000e-02 3.89610390e-02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.24890615, 0.45547281, 0.29562105])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.optimize(devData, devAns, 1000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.updateLambdas([0.24890615, 0.45547281, 0.29562105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this predictor, it predicts the preposition by selecting the one that generates the lowest perplexity in its neighborhood. Unlike traditional prediction that treats the PREP as the next gram and select the prep with the highest probability, in this approach, we concatenate all n-1grams, the candidate prep and the **following 2 grams**, and select the n+2grams with the lowest perplexity (the same as selecting the n+2gram with highest probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of predicting the devData is 0.7131837307152875\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of predicting the devData is\", predictor.performace(devData, devAns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test result\n",
    "with open(\"test.in\", 'r') as testFile:\n",
    "    testData = [line.strip() for line in testFile]\n",
    "\n",
    "predictor.predictData(testData, \"test.out\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
